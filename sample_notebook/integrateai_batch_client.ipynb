{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# integrate.ai API Sample Notebook to run client on AWS Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment variables (or replace inline) with your IAI credentials\n",
    "### Generate and manage this token in the UI, in the Tokens page\n",
    "### Generate AWS session credentials or use the default profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "get access token"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IAI_TOKEN = os.environ.get(\"IAI_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate to the integrate.ai api client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "authenticate to client"
    ]
   },
   "outputs": [],
   "source": [
    "from integrate_ai_sdk.api import connect\n",
    "\n",
    "client = connect(token=IAI_TOKEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Specifying optional AWS Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your AWS Credentials if you are generating temporary ones, else use the default profile credentials\n",
    "aws_creds = {\n",
    "    \"ACCESS_KEY\": os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n",
    "    \"SECRET_KEY\": os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    \"SESSION_TOKEN\": os.environ.get(\"AWS_SESSION_TOKEN\"),\n",
    "    \"REGION\": os.environ.get(\"AWS_REGION\"),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying path to datasets and batch job definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data path in s3\n",
    "train_path1 = \"s3://iai-client.sample-data-e2e.integrate.ai/train_silo0.parquet\"\n",
    "train_path2 = \"s3://iai-client.sample-data-e2e.integrate.ai/train_silo1.parquet\"\n",
    "test_path = \"s3://iai-client.sample-data-e2e.integrate.ai/test.parquet\"\n",
    "# Specify the name of your job_queue, job_definition\n",
    "job_queue = \"iai-client-batch-job-queue\"\n",
    "job_def = \"iai-client-batch-job\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batch task builder object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "import task builder"
    ]
   },
   "outputs": [],
   "source": [
    "from integrate_ai_sdk.taskgroup.taskbuilder import aws as taskbuilder_aws\n",
    "from integrate_ai_sdk.taskgroup.base import SessionTaskGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "create task builder"
    ]
   },
   "outputs": [],
   "source": [
    "tb = taskbuilder_aws.batch(job_queue=job_queue, aws_credentials=aws_creds, cpu_job_definition=job_def)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an EDA Session for exploring the datasets\n",
    "\n",
    "To create an EDA session, we specify a `dataset_config` dictionary indicating the columns to explore for each dataset. Here the empty list `[]` means to include all columns. The number of expected datasets will be inferred as the number of items in dataset_config (i.e., two). Alternatively, we can manually set it with the optional argument `num_datasets` in `client.create_eda_session()`\n",
    "\n",
    "For information more information on how to configure an EDA session from scratch, reference the documentation [here](https://integrate-ai.gitbook.io/integrate.ai-user-documentation/tutorials/exploratory-data-analysis-eda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "assign dataset config"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_config = {\"dataset_one\": [], \"dataset_two\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start eda session"
    ]
   },
   "outputs": [],
   "source": [
    "eda_session = client.create_eda_session(\n",
    "    name=\"Testing notebook - EDA\",\n",
    "    description=\"I am testing EDA session creation through a notebook\",\n",
    "    data_config=dataset_config,\n",
    ").start()\n",
    "\n",
    "eda_session.id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run EDA Client jobs on AWS Batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create task_group with appropriate number of tasks\n",
    "#### Number of tasks added should match number of datasets specified when creating the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start eda batch clients"
    ]
   },
   "outputs": [],
   "source": [
    "eda_task_group_context = (\n",
    "    SessionTaskGroup(eda_session)\n",
    "    .add_task(tb.eda(dataset_name=\"dataset_one\", dataset_path=train_path1, vcpus=\"2\", memory=\"16384\", client=client))\n",
    "    .add_task(tb.eda(dataset_name=\"dataset_two\", dataset_path=train_path2, vcpus=\"2\", memory=\"16384\", client=client))\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poll for status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_task_group_context.wait(150)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Session Complete!\n",
    "Now you can analyze the datasets.\n",
    "\n",
    "The results object is a dataset collection, which is comprised of multiple datasets that can be retrieved by name. \n",
    "\n",
    "Each dataset is comprised of columns, which can be retrieved by column name. \n",
    "\n",
    "The same base analysis functions can be performed at the collection, dataset, or column level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "view eda session results"
    ]
   },
   "outputs": [],
   "source": [
    "results = eda_session.results()[\"dataset_one\", \"dataset_two\"]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "get shape of results"
    ]
   },
   "outputs": [],
   "source": [
    "results.mean().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "get info from col"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_one = eda_session.results()[\"dataset_one\"]\n",
    "dataset_one_count = dataset_one[\"x0\"].count()\n",
    "dataset_one[\"x0\"].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram plots can be created using the `.plot_hist()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "plot histogram"
    ]
   },
   "outputs": [],
   "source": [
    "single_hist = dataset_one[\"x0\"].plot_hist()\n",
    "\n",
    "# single_hist.legends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "get histo properties"
    ]
   },
   "outputs": [],
   "source": [
    "# grab histogram data for testing\n",
    "bars = single_hist.gca().patches\n",
    "\n",
    "# Best Estimate of Mean for a histogram: Î£m*n / N\n",
    "# m: The midpoint of the bin\n",
    "# n: The frequency of the bin\n",
    "# N: The total sample size\n",
    "\n",
    "totalSumOfHisto = 0\n",
    "for i in range(len(bars)):\n",
    "    totalSumOfHisto += (bars[i].get_xy()[0] + bars[i].get_width() / 2) * bars[i].get_height()\n",
    "estimatedAvg = totalSumOfHisto / dataset_one_count\n",
    "print(estimatedAvg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Training Session\n",
    "\n",
    "The documentation for [creating a session](https://integrate-ai.gitbook.io/integrate.ai-user-documentation/tutorials/end-user-tutorials/model-training-with-a-sample-local-dataset#create-and-start-the-session) gives a bit more context into the parameters that are used during training session creation.<br />\n",
    "For this session we are going to be using two training clients and two rounds. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample model config and data schema\n",
    "You can find the model config and data schema in the [integrate.ai end user tutorial](https://integrate-ai.gitbook.io/integrate.ai-user-documentation/tutorials/end-user-tutorials/model-training-with-a-sample-local-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "assign schemas"
    ]
   },
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"experiment_name\": \"test_synthetic_tabular\",\n",
    "    \"experiment_description\": \"test_synthetic_tabular\",\n",
    "    \"strategy\": {\"name\": \"FedAvg\", \"params\": {}},\n",
    "    \"model\": {\"params\": {\"input_size\": 15, \"hidden_layer_sizes\": [6, 6, 6], \"output_size\": 2}},\n",
    "    \"balance_train_datasets\": False,\n",
    "    \"ml_task\": {\n",
    "        \"type\": \"classification\",\n",
    "        \"params\": {\n",
    "            \"loss_weights\": None,\n",
    "        },\n",
    "    },\n",
    "    \"optimizer\": {\"name\": \"SGD\", \"params\": {\"learning_rate\": 0.2, \"momentum\": 0.0}},\n",
    "    \"differential_privacy_params\": {\"epsilon\": 4, \"max_grad_norm\": 7},\n",
    "    \"save_best_model\": {\n",
    "        \"metric\": \"loss\",  # to disable this and save model from the last round, set to None\n",
    "        \"mode\": \"min\",\n",
    "    },\n",
    "    \"seed\": 23,  # for reproducibility\n",
    "}\n",
    "\n",
    "data_schema = {\n",
    "    \"predictors\": [\"x0\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\", \"x10\", \"x11\", \"x12\", \"x13\", \"x14\"],\n",
    "    \"target\": \"y\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start session"
    ]
   },
   "outputs": [],
   "source": [
    "training_session = client.create_fl_session(\n",
    "    name=\"Testing notebook\",\n",
    "    description=\"I am testing session creation through a notebook\",\n",
    "    min_num_clients=2,\n",
    "    num_rounds=2,\n",
    "    package_name=\"iai_ffnet\",\n",
    "    model_config=model_config,\n",
    "    data_config=data_schema,\n",
    ").start()\n",
    "\n",
    "training_session.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Client jobs on AWS Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create task_group with appropriate number of tasks\n",
    "#### Number of tasks added should match min_number of clients specified when creating the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "start batch clients"
    ]
   },
   "outputs": [],
   "source": [
    "task_group_context = (\n",
    "    SessionTaskGroup(training_session)\n",
    "    .add_task(tb.hfl(train_path=train_path1, test_path=test_path, vcpus=\"2\", memory=\"16384\", client=client))\n",
    "    .add_task(tb.hfl(train_path=train_path2, test_path=test_path, vcpus=\"2\", memory=\"16384\", client=client))\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor submitted jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session available in group context after submission\n",
    "print(task_group_context.session.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# status of tasks submitted\n",
    "task_group_status = task_group_context.status()\n",
    "for task_status in task_group_status:\n",
    "    print(task_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to monitor if a session has completed successfully or has failed\n",
    "# You can modify the time to wait as per your specific task\n",
    "task_group_context.wait(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Complete!\n",
    "Now you can view the training metrics and start making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "view session metrics"
    ]
   },
   "outputs": [],
   "source": [
    "training_session.metrics().as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = training_session.metrics().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained model parameters are accessible from the completed session\n",
    "\n",
    "Model parameters can be retrieved using the model's state_dict method. These parameters can then be saved with torch.save()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save model"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = training_session.model().as_pytorch()\n",
    "\n",
    "save_state_dict_folder = \"./saved_models\"\n",
    "# PyTorch conventional file type\n",
    "file_name = f\"{training_session.id}.pt\"\n",
    "os.makedirs(save_state_dict_folder, exist_ok=True)\n",
    "saved_state_dict_path = os.path.join(save_state_dict_folder, file_name)\n",
    "\n",
    "with open(saved_state_dict_path, \"w\") as f:\n",
    "    torch.save(model.state_dict(), saved_state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the saved model\n",
    "\n",
    "To load a model saved previously, a model object needs to be initialized first. This can be done by directly importing one of the IAI-supported packages (e.g., FFNet) or using the model class defined in a custom package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "load model"
    ]
   },
   "outputs": [],
   "source": [
    "from integrate_ai_sdk.packages.FFNet.nn_model import FFNet\n",
    "\n",
    "model = FFNet(input_size=15, output_size=2, hidden_layer_sizes=[6, 6, 6])\n",
    "\n",
    "# use torch.load to unpickle the state_dict\n",
    "target_state_dict = torch.load(saved_state_dict_path)\n",
    "\n",
    "model.load_state_dict(target_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_parquet(\"./test.parquet\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert test data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.tensor(test_data[\"y\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(\n",
    "    test_data[[\"x0\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\", \"x10\", \"x11\", \"x12\", \"x13\", \"x14\"]].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "predict"
    ]
   },
   "outputs": [],
   "source": [
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model(X).max(dim=1)[1]\n",
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9668bd68a45449a603c007a3ac0147bfe97ff582415f6f9325554aa5d85a2bfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
